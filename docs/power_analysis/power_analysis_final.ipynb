{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5defcf6-b522-4f46-835b-dacdfd71ad22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from statsmodels.stats.proportion import proportions_ztest, proportions_ztost, test_proportions_2indep\n",
    "#from statsmodels.stats.nonparametric import rank_compare_2indep\n",
    "#from statsmodels.stats.weightstats import ttest_ind, ttost_ind\n",
    "from scipy.stats import pearsonr, spearmanr, mannwhitneyu, ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a2aed0-c9a9-4f1a-b4ee-fae8985bb9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a3d7f9-e693-4e67-99fc-4295c45fa42d",
   "metadata": {},
   "source": [
    "First, using ALTO results, let's get the diff in probability we need to be sensitive to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d410a95b-94cf-46e6-8bff-c8b321b65a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:05<00:00, 198.35it/s]\n"
     ]
    }
   ],
   "source": [
    "all_pps = list(np.arange(0, 1, 0.001))\n",
    "all_purities = []\n",
    "num_docs = 200\n",
    "num_topics = 20\n",
    "l_topics = list(np.arange(num_topics) + 1)\n",
    "\n",
    "for pp in tqdm(all_pps):\n",
    "    probs_list = [pp] + [(1 - pp)/(num_topics - 1)]*(num_topics - 1)\n",
    "    ground_truth = np.random.randint(num_topics, size=num_docs) + 1\n",
    "    \n",
    "    annotator_vec = []\n",
    "    for doc_i in range(num_docs):\n",
    "        gt = ground_truth[doc_i]\n",
    "        annotator_vec.append(np.random.choice([gt] + l_topics[:(gt-1)] + l_topics[gt:], \n",
    "                                        p=probs_list))\n",
    "    all_purities.append(purity_score(ground_truth, annotator_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6f4125-6db8-480f-8b1c-bbb55d400204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob of binning a doc correctly for better condition that gives closest result to ALTO = 0.355\n"
     ]
    }
   ],
   "source": [
    "alto_b_purity = 0.35371\n",
    "b_diffs = []\n",
    "for p in all_purities:\n",
    "    b_diffs.append(abs(p - alto_b_purity))\n",
    "b_ind = np.argmin(b_diffs)\n",
    "selected_b_purity = all_purities[b_ind]\n",
    "print('Prob of binning a doc correctly for better condition that gives closest result to ALTO = ' + str(selected_b_purity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e54f67-71ec-441f-8452-51e13134c4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob of binning a doc correctly for worse condition that gives closest result to ALTO = 0.325\n"
     ]
    }
   ],
   "source": [
    "alto_w_purity = 0.32384\n",
    "w_diffs = []\n",
    "for p in all_purities:\n",
    "    w_diffs.append(abs(p - alto_w_purity))\n",
    "w_ind = np.argmin(w_diffs)\n",
    "selected_w_purity = all_purities[w_ind]\n",
    "print('Prob of binning a doc correctly for worse condition that gives closest result to ALTO = ' + str(selected_w_purity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1495ceb9-0ee6-4090-a9ea-cdf4063c1b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prob diff we want to be sensitive to = 0.02999999999999997\n"
     ]
    }
   ],
   "source": [
    "prob_diff = selected_b_purity - selected_w_purity\n",
    "print('Prob diff we want to be sensitive to = ' + str(prob_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb9b0e8d-d50d-4e24-8174-bb6fc885720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bin_array(topics, questions=30):\n",
    "    selections = np.zeros(topics)\n",
    "    selections[:questions]  = 1\n",
    "    np.random.shuffle(selections)\n",
    "    return selections\n",
    "\n",
    "def permutation_test(a, b, alternative=\"two-sided\", value=0, iters=1000):\n",
    "    og_diff = np.mean(a) - np.mean(b) - value\n",
    "    combin = np.concatenate([a, b])\n",
    "    n_a, n_b = len(a), len(b)\n",
    "    diffs = [\n",
    "        np.mean(samp[:n_a]) - np.mean(samp[n_a:])\n",
    "        for _ in range(iters)\n",
    "        for samp in [np.random.choice(combin, n_a + n_b, replace=False)]\n",
    "    ]\n",
    "    if alternative == \"two-sided\":\n",
    "        return og_diff, np.mean(np.abs(og_diff) < np.abs(diffs)), diffs\n",
    "    elif alternative == \"larger\":\n",
    "        return og_diff, np.mean(og_diff < diffs), diffs\n",
    "    elif alternative == \"smaller\":\n",
    "        return og_diff, np.mean(og_diff > diffs), diffs\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be one of (two-sided, larger, smaller)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b0a895d-b90e-447a-ad3c-eeab5ebd1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sims(num_annotators, prob_diff):   \n",
    "    \n",
    "    pvals = []\n",
    "    num_docs = 200 #len(set(nist_ras_data['doc_id'])) #assuming a sample of all docs is shown?\n",
    "    num_topics = 20\n",
    "    l_topics = list(np.arange(num_topics) + 1)\n",
    "    \n",
    "    #prob_diff = 0.1 #effect size - probability difference between the two scenarios for picking right label \n",
    "    num_iters = 10000\n",
    "\n",
    "    np.random.seed(454)\n",
    "    for _ in tqdm(range(num_iters)):\n",
    "        pp_b = np.random.uniform(0.1, 1)\n",
    "        probs_list_b = [pp_b] + [(1 - pp_b)/(num_topics - 1)]*(num_topics - 1)\n",
    "        pp_w = pp_b - prob_diff\n",
    "        probs_list_w = [pp_w] + [(1 - pp_w)/(num_topics - 1)]*(num_topics - 1)\n",
    "        \n",
    "        ground_truth = np.random.randint(num_topics, size=num_docs) + 1\n",
    "        \n",
    "        s1_vals, s2_vals = [], []\n",
    "        for _ in range(num_annotators):\n",
    "            a_1_vec, a_2_vec = [], []\n",
    "            for doc_i in range(num_docs):\n",
    "                gt = ground_truth[doc_i]\n",
    "                a_1_vec.append(np.random.choice([gt] + l_topics[:(gt-1)] + l_topics[gt:], \n",
    "                                                p=probs_list_b))\n",
    "                a_2_vec.append(np.random.choice([gt] + l_topics[:(gt-1)] + l_topics[gt:], \n",
    "                                                p=probs_list_w))\n",
    "            s1_vals.append(purity_score(ground_truth, a_1_vec))\n",
    "            s2_vals.append(purity_score(ground_truth, a_2_vec))\n",
    "        \n",
    "        stat, pval, _ = permutation_test(s1_vals, s2_vals, alternative = 'larger')\n",
    "        #stat, pval = mannwhitneyu(s1_vals, s2_vals, alternative = 'greater')\n",
    "        pvals.append(pval)\n",
    "    return pvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffac4fb3-d260-4b81-bd44-c5413646a8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 4793/10000 [17:38<19:12,  4.52it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvals_20 = np.array(run_sims(num_annotators = 20, prob_diff = prob_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7c5feb8-e6f2-4616-adf7-d7b7084a4287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2999/10000 [13:18<30:59,  3.77it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvals_25 = np.array(run_sims(num_annotators = 25, prob_diff = prob_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3cd5987-27e8-4d04-ad26-8e847776c8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 2999/10000 [15:30<36:06,  3.23it/s]IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pvals_30 = np.array(run_sims(num_annotators = 30, prob_diff = prob_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70c8ab7f-a10c-491e-b1dc-2f42e0e19037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Number of Annotators = 20\n",
      "alpha  0.05, power: 0.855\n"
     ]
    }
   ],
   "source": [
    "print('For Number of Annotators = 20')\n",
    "alpha = 0.05\n",
    "print(f\"alpha {alpha:5}, power: {np.mean(pvals_20 < alpha):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "244b58cf-0c6e-4ed2-930a-e8385f11ff7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Number of Annotators = 25\n",
      "alpha  0.05, power: 0.910\n"
     ]
    }
   ],
   "source": [
    "print('For Number of Annotators = 25')\n",
    "alpha = 0.05\n",
    "print(f\"alpha {alpha:5}, power: {np.mean(pvals_25 < alpha):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e7d5cb7-4be2-4e3e-950d-b7027fca156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Number of Annotators = 30\n",
      "alpha  0.05, power: 0.938\n"
     ]
    }
   ],
   "source": [
    "print('For Number of Annotators = 30')\n",
    "alpha = 0.05\n",
    "print(f\"alpha {alpha:5}, power: {np.mean(pvals_30 < alpha):0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99d3ce-6425-48d7-a25d-8ebd44e65138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tbip] *",
   "language": "python",
   "name": "conda-env-tbip-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
